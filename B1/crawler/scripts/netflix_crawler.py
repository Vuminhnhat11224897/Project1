"""
Netflix Data Crawler
Crawler thu th·∫≠p d·ªØ li·ªáu phim t·ª´ TMDB API
"""

import os
import json
import time
import logging
import argparse
from datetime import datetime
from typing import Dict, List, Optional, Set, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
import pandas as pd
import hashlib
from logging.handlers import RotatingFileHandler
# Th√™m c√°c import c·∫ßn thi·∫øt
try:
    # Import t·ª´ th∆∞ m·ª•c hi·ªán t·∫°i khi ch·∫°y tr·ª±c ti·∫øp
    from config import Config
    from tmdb_client import TMDBClient
    from cache_manager import CacheManager
except ImportError:
    # Import ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi khi ch·∫°y trong Docker/Airflow
    import sys
    script_dir = os.path.dirname(os.path.abspath(__file__))
    sys.path.insert(0, script_dir)
    from config import Config
    from tmdb_client import TMDBClient
    from cache_manager import CacheManager


class NetflixDataCrawler:
    """
    Crawler ch√≠nh ƒë·ªÉ thu th·∫≠p d·ªØ li·ªáu phim t·ª´ TMDB API.
    """
    
    def __init__(self, config=None):
        """
        Kh·ªüi t·∫°o Netflix data crawler.
        
        Args:
            config: Config object ho·∫∑c None ƒë·ªÉ t·∫°o m·ªõi t·ª´ m·∫∑c ƒë·ªãnh
        """
        # S·ª≠ d·ª•ng c·∫•u h√¨nh ƒë∆∞·ª£c cung c·∫•p ho·∫∑c t·∫°o m·ªõi t·ª´ m·∫∑c ƒë·ªãnh
        self.config = config if config else Config()
        
        # Kh·ªüi t·∫°o TMDB client
        self.tmdb = TMDBClient(self.config)
        
        # Kh·ªüi t·∫°o cache manager
        cache_dir = self.config.get("cache_dir", "cache")
        os.makedirs(cache_dir, exist_ok=True)
        self.cache = CacheManager(cache_dir)
        
        # ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c d·ªØ li·ªáu
        self.raw_data_dir = self.config.get("raw_data_dir")
        self.processed_data_dir = self.config.get("processed_data_dir")
        
        # ƒê·∫£m b·∫£o th∆∞ m·ª•c t·ªìn t·∫°i
        os.makedirs(self.raw_data_dir, exist_ok=True)
        os.makedirs(self.processed_data_dir, exist_ok=True)
        
        # L·∫•y c√°c tham s·ªë c·∫•u h√¨nh
        self.batch_size = self.config.get("batch_size", 25)
        self.max_workers = self.config.get("max_workers", 5)
        self.max_retries = self.config.get("max_retries", 3)
        self.retry_backoff = self.config.get("retry_backoff", 1.5)
        self.rate_limit_wait = self.config.get("rate_limit_wait", 0.25)
        
        logging.info(f"Netflix Data Crawler ƒë√£ kh·ªüi t·∫°o: raw_data_dir={self.raw_data_dir}")
    
    def with_retry(self, func, *args, max_retries=None, backoff_factor=None, **kwargs):
        """
        Th·ª±c hi·ªán API call v·ªõi c∆° ch·∫ø retry t·ª± ƒë·ªông
        
        Args:
            func: Ph∆∞∆°ng th·ª©c API c·∫ßn g·ªçi
            max_retries: S·ªë l·∫ßn th·ª≠ l·∫°i t·ªëi ƒëa (None = d√πng c·∫•u h√¨nh)
            backoff_factor: H·ªá s·ªë tƒÉng th·ªùi gian ch·ªù (None = d√πng c·∫•u h√¨nh)
            
        Returns:
            K·∫øt qu·∫£ t·ª´ API call ho·∫∑c None n·∫øu t·∫•t c·∫£ c√°c l·∫ßn th·ª≠ ƒë·ªÅu th·∫•t b·∫°i
        """
        if max_retries is None:
            max_retries = self.max_retries
            
        if backoff_factor is None:
            backoff_factor = self.retry_backoff
            
        retries = 0
        last_exception = None
        
        while retries < max_retries:
            try:
                return func(*args, **kwargs)
            except Exception as e:
                last_exception = e
                wait_time = backoff_factor * (2 ** retries)
                logging.warning(f"API call failed: {e}. Retrying in {wait_time:.1f}s ({retries+1}/{max_retries})")
                time.sleep(wait_time)
                retries += 1
        
        logging.error(f"API call failed after {max_retries} retries: {last_exception}")
        return None
    
    def crawl_movie_list(self, list_type: str, num_pages: int = 5) -> List[Dict]:
        """
        Crawl danh s√°ch phim theo lo·∫°i (popular, trending, netflix, etc).
        
        Args:
            list_type: Lo·∫°i danh s√°ch phim (netflix, popular, trending_day, trending_week, top_rated)
            num_pages: S·ªë trang ƒë·ªÉ crawl
            
        Returns:
            List[Dict]: Danh s√°ch phim ƒë√£ crawl
        """
        logging.info(f"Crawling {list_type} movies - {num_pages} pages")
        
        all_movies = []
        
        for page in range(1, num_pages + 1):
            logging.info(f"Crawling {list_type} page {page}/{num_pages}")
            
            if list_type == "popular":
                movies = self.with_retry(self.tmdb.get_popular_movies, page)
            elif list_type == "top_rated":
                movies = self.with_retry(self.tmdb.get_top_rated_movies, page)
            elif list_type == "now_playing":
                movies = self.with_retry(self.tmdb.get_now_playing_movies, page)
            elif list_type == "netflix":
                movies = self.with_retry(self.tmdb.get_netflix_movies, page)
            elif list_type == "trending_day":
                movies = self.with_retry(self.tmdb.get_trending_movies, "day", page)
            elif list_type == "trending_week":
                movies = self.with_retry(self.tmdb.get_trending_movies, "week", page)
            else:
                logging.warning(f"Unknown list type: {list_type}")
                continue
            
            if movies and 'results' in movies:
                for movie in movies['results']:
                    movie['source_list'] = list_type
                    movie['page'] = page
                all_movies.extend(movies['results'])
                logging.info(f"Found {len(movies['results'])} movies")
            
            time.sleep(self.rate_limit_wait)  # Rate limiting
        
        logging.info(f"Total {list_type} movies: {len(all_movies)}")
        return all_movies
    
    def crawl_movie_details(self, movie_id: int) -> Dict:
        """
        Crawl th√¥ng tin chi ti·∫øt c·ªßa m·ªôt phim v·ªõi cache.
        
        Args:
            movie_id: ID c·ªßa phim
            
        Returns:
            Dict: Th√¥ng tin chi ti·∫øt c·ªßa phim
        """
        logging.info(f"Crawling details for movie ID: {movie_id}")
        
        # Ki·ªÉm tra cache
        cache_key = f"movie_details_{movie_id}"
        cached_data = self.cache.get(cache_key)
        
        if cached_data:
            logging.info(f"Using cached data for movie ID: {movie_id}")
            return cached_data
        
        movie_data = {
            'movie_id': movie_id,
            'crawl_timestamp': datetime.now().isoformat(),
            'details': None,
            'credits': None,
            'keywords': None,
            'videos': None,
            'reviews': None,
            'similar': None
        }
        
        # 1. Movie details
        movie_details = self.with_retry(self.tmdb.get_movie_details, movie_id)
        if movie_details:
            movie_data['details'] = movie_details
            logging.info(f"‚úì Details: {movie_details.get('title', 'N/A')}")
        else:
            logging.warning(f"‚úó No details found for movie ID: {movie_id}")
            return movie_data  # Return early if we can't get basic details
        
        # 2. Credits (cast & crew)
        credits = self.with_retry(self.tmdb.get_movie_credits, movie_id)
        if credits:
            movie_data['credits'] = credits
            cast_count = len(credits.get('cast', []))
            crew_count = len(credits.get('crew', []))
            logging.info(f"‚úì Credits: {cast_count} cast, {crew_count} crew")
        
        # 3. Keywords
        keywords = self.with_retry(self.tmdb.get_movie_keywords, movie_id)
        if keywords:
            movie_data['keywords'] = keywords
            keyword_count = len(keywords.get('keywords', []))
            logging.info(f"‚úì Keywords: {keyword_count} keywords")
        
        # 4. Videos/Trailers
        videos = self.with_retry(self.tmdb.get_movie_videos, movie_id)
        if videos:
            movie_data['videos'] = videos
            video_count = len(videos.get('results', []))
            logging.info(f"‚úì Videos: {video_count} videos")
        
        # 5. Reviews
        reviews = self.with_retry(self.tmdb.get_movie_reviews, movie_id)
        if reviews:
            movie_data['reviews'] = reviews
            review_count = len(reviews.get('results', []))
            logging.info(f"‚úì Reviews: {review_count} reviews")
        
        # 6. Similar movies
        similar = self.with_retry(self.tmdb.get_movie_similar, movie_id)
        if similar:
            movie_data['similar'] = similar
            similar_count = len(similar.get('results', []))
            logging.info(f"‚úì Similar: {similar_count} similar movies")
        
        # L∆∞u v√†o cache n·∫øu c√≥ d·ªØ li·ªáu c∆° b·∫£n
        if movie_data['details']:
            self.cache.set(cache_key, movie_data)
        
        return movie_data
    
    def save_movie_batch(self, movies, timestamp, batch_index):
        """
        L∆∞u m·ªôt batch phim v√†o file ri√™ng bi·ªát
        
        Args:
            movies: Danh s√°ch phim c·∫ßn l∆∞u
            timestamp: Timestamp cho t√™n file
            batch_index: Ch·ªâ s·ªë batch
            
        Returns:
            str: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file ƒë√£ l∆∞u
        """
        if not movies:
            return None
            
        batch_file = os.path.join(self.raw_data_dir, 
                                  f"detailed_movies_batch_{batch_index}_{timestamp}.json")
        self.save_json(movies, batch_file)
        logging.info(f"Saved batch {batch_index}: {len(movies)} movies")
        return batch_file
    
    def crawl_daily_netflix_data(self, 
                                num_pages_per_source: int = None,
                                max_movies: int = None,
                                sources: List[str] = None,
                                resume_from: str = None) -> Dict:
        """
        Crawl d·ªØ li·ªáu h√†ng ng√†y t·ª´ nhi·ªÅu ngu·ªìn.
        
        Args:
            num_pages_per_source: S·ªë trang m·ªói ngu·ªìn (None = d√πng c·∫•u h√¨nh)
            max_movies: S·ªë phim t·ªëi ƒëa ƒë·ªÉ crawl chi ti·∫øt (None = d√πng c·∫•u h√¨nh)
            sources: Danh s√°ch ngu·ªìn ƒë·ªÉ crawl (None = d√πng t·∫•t c·∫£ ngu·ªìn ƒë∆∞·ª£c b·∫≠t trong c·∫•u h√¨nh)
            resume_from: ƒê∆∞·ªùng d·∫´n file l∆∞u danh s√°ch movie_ids ƒë√£ th·∫•t b·∫°i ƒë·ªÉ th·ª≠ l·∫°i
            
        Returns:
            Dict: K·∫øt qu·∫£ crawl v·ªõi summary v√† danh s√°ch phim
        """
        # S·ª≠ d·ª•ng c·∫•u h√¨nh m·∫∑c ƒë·ªãnh n·∫øu kh√¥ng ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
        if num_pages_per_source is None:
            num_pages_per_source = self.config.get("max_pages_per_source", 5)
            
        if max_movies is None:
            max_movies = self.config.get("max_movies_per_day", 100)
        
        # N·∫øu kh√¥ng ch·ªâ ƒë·ªãnh ngu·ªìn, s·ª≠ d·ª•ng t·∫•t c·∫£ ngu·ªìn ƒë∆∞·ª£c b·∫≠t trong c·∫•u h√¨nh
        if sources is None:
            sources = []
            data_sources = self.config.get("data_sources", {})
            for source, config in data_sources.items():
                if config.get("enabled", True):
                    sources.append(source)
                    
        logging.info(f"Starting daily Netflix data crawl:")
        logging.info(f"- Sources: {sources}")
        logging.info(f"- Pages per source: {num_pages_per_source}")
        logging.info(f"- Max movies for detailed crawl: {max_movies}")
        
        # T·∫°o timestamp cho t√™n file
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # N·∫øu l√† ch·∫ø ƒë·ªô ti·∫øp t·ª•c, load movie IDs ƒë√£ th·∫•t b·∫°i
        selected_movies = []
        if resume_from and os.path.exists(resume_from):
            try:
                with open(resume_from, 'r') as f:
                    failed_data = json.load(f)
                    selected_movies = failed_data.get('failed_movie_ids', [])
                    logging.info(f"Resuming crawl for {len(selected_movies)} previously failed movies")
            except Exception as e:
                logging.error(f"Error loading resume file {resume_from}: {e}")
                # Ti·∫øp t·ª•c v·ªõi quy tr√¨nh b√¨nh th∆∞·ªùng
                selected_movies = []
        
        # N·∫øu kh√¥ng ph·∫£i ch·∫ø ƒë·ªô resume ho·∫∑c resume th·∫•t b·∫°i, thu th·∫≠p movie IDs m·ªõi
        if not selected_movies:
            # 1. Thu th·∫≠p movie IDs t·ª´ c√°c ngu·ªìn kh√°c nhau
            all_movie_ids = set()
            movie_sources = {}
            
            for source in sources:
                try:
                    # ƒêi·ªÅu ch·ªânh s·ªë trang theo c·∫•u h√¨nh ngu·ªìn c·ª• th·ªÉ n·∫øu c√≥
                    source_pages = self.config.get_nested("data_sources", source, "pages", default=num_pages_per_source)
                    movies = self.crawl_movie_list(source, source_pages)
                    
                    for movie in movies:
                        movie_id = movie['id']
                        all_movie_ids.add(movie_id)
                        
                        if movie_id not in movie_sources:
                            movie_sources[movie_id] = []
                        movie_sources[movie_id].append(source)
                        
                except Exception as e:
                    logging.error(f"Error crawling {source}: {e}")
                    continue
            
            logging.info(f"Collected {len(all_movie_ids)} unique movies from {len(sources)} sources")
            
            # 2. L∆∞u summary danh s√°ch phim
            movie_list_summary = {
                'crawl_date': datetime.now().isoformat(),
                'total_unique_movies': len(all_movie_ids),
                'sources': sources,
                'pages_per_source': num_pages_per_source,
                'movie_sources': movie_sources
            }
            
            summary_file = os.path.join(self.raw_data_dir, f"movie_list_summary_{timestamp}.json")
            self.save_json(movie_list_summary, summary_file)
            
            # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng phim ƒë·ªÉ crawl chi ti·∫øt
            selected_movies = list(all_movie_ids)[:max_movies]
        
        logging.info(f"Crawling detailed info for {len(selected_movies)} movies...")
        
        # Kh·ªüi t·∫°o file dataset cho to√†n b·ªô k·∫øt qu·∫£
        dataset_header = {
            'crawl_summary': {
                'crawl_date': datetime.now().isoformat(),
                'crawl_timestamp': timestamp,
                'total_movies_attempted': len(selected_movies),
                'sources': sources if not resume_from else "resume"
            },
            'movies': []
        }
        
        final_file = os.path.join(self.raw_data_dir, f"netflix_raw_dataset_{timestamp}.json")
        self.save_json(dataset_header, final_file)
        
        detailed_movies = []
        failed_movies = []
        batch_buffer = []
        
        # 3. Crawl th√¥ng tin chi ti·∫øt v·ªõi x·ª≠ l√Ω song song
        max_workers = min(self.max_workers, len(selected_movies))
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Submit all tasks
            future_to_id = {executor.submit(self.crawl_movie_details, movie_id): movie_id for movie_id in selected_movies}
            
            # Process results as they complete
            for i, future in enumerate(as_completed(future_to_id), 1):
                movie_id = future_to_id[future]
                try:
                    movie_data = future.result()
                    
                    if movie_data['details']:  # Only save if we got basic details
                        if not resume_from:  # N·∫øu kh√¥ng ph·∫£i resume, th√™m th√¥ng tin ngu·ªìn
                            movie_data['sources'] = movie_sources.get(movie_id, [])
                        detailed_movies.append(movie_data)
                        batch_buffer.append(movie_data)
                    else:
                        if movie_id not in failed_movies:  # Tr√°nh th√™m tr√πng l·∫∑p
                            failed_movies.append(movie_id)
                    
                    # Progress save every batch_size movies
                    if len(batch_buffer) >= self.batch_size or i == len(selected_movies):
                        batch_file = self.save_movie_batch(batch_buffer, timestamp, i // self.batch_size)
                        batch_buffer = []  # Reset buffer
                        
                except Exception as e:
                    logging.error(f"Error crawling movie {movie_id}: {e}")
                    if movie_id not in failed_movies:  # Tr√°nh th√™m tr√πng l·∫∑p
                        failed_movies.append(movie_id)
                    continue
        
        # 4. L∆∞u k·∫øt qu·∫£ cu·ªëi c√πng
        final_results = {
            'crawl_summary': {
                'crawl_date': datetime.now().isoformat(),
                'crawl_timestamp': timestamp,
                'total_movies_attempted': len(selected_movies),
                'successful_crawls': len(detailed_movies),
                'failed_crawls': len(failed_movies),
                'failed_movie_ids': failed_movies
            },
            'movies': detailed_movies
        }
        
        self.save_json(final_results, final_file)
        
        # L∆∞u ri√™ng danh s√°ch phim th·∫•t b·∫°i ƒë·ªÉ c√≥ th·ªÉ retry sau
        if failed_movies:
            failed_file = os.path.join(self.raw_data_dir, f"failed_movies_{timestamp}.json")
            self.save_json({'failed_movie_ids': failed_movies}, failed_file)
            logging.info(f"Saved {len(failed_movies)} failed movie IDs to {failed_file}")
        
        logging.info(f"CRAWL COMPLETED!")
        logging.info(f"- Successfully crawled: {len(detailed_movies)} movies")
        logging.info(f"- Failed: {len(failed_movies)} movies")
        logging.info(f"- Saved to: {final_file}")
        
        return final_results
    
    def save_json(self, data, filepath):
        """
        L∆∞u d·ªØ li·ªáu v√†o file JSON v·ªõi x·ª≠ l√Ω l·ªói.
        
        Args:
            data: D·ªØ li·ªáu ƒë·ªÉ l∆∞u
            filepath: ƒê∆∞·ªùng d·∫´n file ƒë·ªÉ l∆∞u
            
        Returns:
            bool: True n·∫øu th√†nh c√¥ng, False n·∫øu th·∫•t b·∫°i
        """
        try:
            # ƒê·∫£m b·∫£o th∆∞ m·ª•c t·ªìn t·∫°i
            os.makedirs(os.path.dirname(filepath), exist_ok=True)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            # L·∫•y k√≠ch th∆∞·ªõc file
            file_size = os.path.getsize(filepath)
            logging.info(f"Saved to {filepath} ({file_size:,} bytes)")
            return True
            
        except Exception as e:
            logging.error(f"Error saving {filepath}: {e}")
            return False
    
    def load_latest_raw_data(self):
        """
        Load file raw data m·ªõi nh·∫•t.
        
        Returns:
            Dict: D·ªØ li·ªáu t·ª´ file m·ªõi nh·∫•t ho·∫∑c None n·∫øu kh√¥ng t√¨m th·∫•y
        """
        if not os.path.exists(self.raw_data_dir):
            logging.error(f"Raw data directory not found: {self.raw_data_dir}")
            return None
        
        # T√¨m file m·ªõi nh·∫•t
        raw_files = []
        for filename in os.listdir(self.raw_data_dir):
            if filename.startswith("netflix_raw_dataset_") and filename.endswith(".json"):
                filepath = os.path.join(self.raw_data_dir, filename)
                creation_time = os.path.getctime(filepath)
                raw_files.append((creation_time, filepath))
        
        if not raw_files:
            logging.warning("No raw dataset files found")
            return None
        
        # Load file m·ªõi nh·∫•t
        latest_file = max(raw_files, key=lambda x: x[0])[1]
        logging.info(f"Loading latest raw data: {latest_file}")
        
        try:
            with open(latest_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            logging.info(f"Loaded {len(data.get('movies', []))} movies")
            return data
            
        except Exception as e:
            logging.error(f"Error loading {latest_file}: {e}")
            return None


def setup_logging(log_dir=None, log_level=logging.INFO):
    """
    Configure logging to write to both file and console (safe for Airflow).
    Args:
        log_dir: Folder to store log file
        log_level: Logging level
    Returns:
        logger: Configured logger
    """
    if log_dir is None:
        log_dir = os.environ.get("LOGS_DIR", "logs")

    os.makedirs(log_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(log_dir, f"netflix_crawler_{timestamp}.log")

    # ‚úÖ T·∫°o logger ri√™ng bi·ªát, kh√¥ng d√πng root logger
    logger = logging.getLogger("netflix_crawler")
    logger.setLevel(log_level)
    logger.propagate = False  # ‚õî Kh√¥ng lan l√™n root (Airflow logger)

    # üßπ X√≥a handler c≈© n·∫øu c√≥
    if logger.handlers:
        logger.handlers.clear()

    # File handler
    file_handler = RotatingFileHandler(
        log_file, maxBytes=5 * 1024 * 1024, backupCount=3, encoding='utf-8'
    )
    file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(file_formatter)
    logger.addHandler(file_handler)

    # Console handler (tu·ª≥ ch·ªçn, c√≥ th·ªÉ t·∫Øt n·∫øu ch·∫°y trong Airflow)
    if os.environ.get("DOCKER_ENV", "").lower() != "true":
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(file_formatter)
        logger.addHandler(console_handler)

    # üõë Kh√¥ng d√πng logging.info(...) ·ªü ƒë√¢y ‚Äî thay v√†o ƒë√≥:
    logger.info(f"‚úÖ Logging configured. File: {log_file}")
    return logger


def main():
    """
    Main function to run crawler directly.
    """
    # Parse command line arguments
    parser = argparse.ArgumentParser(description='Netflix Data Crawler')
    parser.add_argument('--resume', help='Resume from failed movies file')
    parser.add_argument('--max-movies', type=int, help='Maximum number of movies to crawl')
    parser.add_argument('--pages', type=int, help='Number of pages per source')
    parser.add_argument('--sources', nargs='+', help='Specific sources to crawl')
    parser.add_argument('--log-dir', help='Directory for log files')
    parser.add_argument('--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'], 
                        default='INFO', help='Logging level')
    args = parser.parse_args()
    
    # Setup logging
    log_level = getattr(logging, args.log_level)
    setup_logging(log_dir=args.log_dir, log_level=log_level)
    
    logging.info("=== NETFLIX CRAWLER STARTING ===")
    
    # Load configuration
    try:
        config = Config()
        logging.info(f"Configuration loaded successfully")
    except Exception as e:
        logging.error(f"Error loading configuration: {e}")
        return
    
    # Create crawler
    try:
        crawler = NetflixDataCrawler(config)
        logging.info("Netflix crawler initialized")
    except Exception as e:
        logging.error(f"Error initializing crawler: {e}")
        return
    
    # Run daily crawl
    try:
        crawler.crawl_daily_netflix_data(
            num_pages_per_source=args.pages,
            max_movies=args.max_movies,
            sources=args.sources,
            resume_from=args.resume
        )
        logging.info("Crawling completed successfully")
    except Exception as e:
        logging.error(f"Error during crawling: {e}")
        return
    
    logging.info("=== NETFLIX CRAWLER FINISHED ===")


if __name__ == "__main__":
    main()